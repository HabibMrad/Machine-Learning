\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Assignment\ \#4}
\newcommand{\hmwkDueDate}{DATE}
\newcommand{\hmwkClass}{Information Theory}
\newcommand{\hmwkClassTime}{}
\newcommand{\hmwkClassInstructor}{Marek Smieja}
\newcommand{\hmwkAuthorName}{\textbf{Szymon Maszke}}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}

  \maketitle

  \pagebreak

  \begin{homeworkProblem}
    Given joint distribution:

      \begin{table}[h]
      \centering
      \caption{Sum of rolls}
      \begin{tabular}{|c|ccccccc}
      $Y/X$& 1 & 2 & 3 & 4 \\
      \hline
      1 & 1/8 & 1/16 & 1/32 & 1/32  \\
      2 & 1/16 & 1/8 & 1/32 & 1/32  \\
      3 & 1/16 & 1/16 & 1/16 & 1/16  \\
      4 & 1/4 & 0 & 0 & 0  \\
      \end{tabular}
      \end{table}

    Calculate $H(X), H(Y), H(X,Y), H(X|Y), H(Y|X), I(X;Y)$\\

    \solution\\

    Entropy of random variable is given by:
    \begin{equation}
      H(X) = \sum_{i}p_i *(-\log_2p_i)
    \end{equation}

    Calculations for each provided below:
    \begin{itemize}
      \item $H(X)$
        \begin{itemize}
          \item We need marginal distribution, given by formula:
            \begin{equation}
              P(X) = \sum_{y}P(X,Y) = \sum_{y}P(X|Y=y)P(Y=y)
            \end{equation}
            to obtain following probability vector:
            \begin{equation}
              [1/2, 1/4, 1/8, 1/8]
            \end{equation}
          \item Entropy of X will be:
            \begin{equation}
              1/2 * -\log_2(1/2) + 1/4 * -\log_2(1/4) + 2/8 * -\log_2(1/8) = 1/2 +
              1/2 + 3/4 = 7/4
            \end{equation}
          \end{itemize}
      \item $H(Y)$
        \begin{itemize}
          \item We need marginal distribution, given by formula:
            \begin{equation}
              P(Y) = \sum_{y}P(X,Y) = \sum_{x}P(Y|X=x)P(X=x)
            \end{equation}
            to obtain following probability vector:
            \begin{equation}
              [1/4, 1/4, 1/4, 1/4]
            \end{equation}
          \item Entropy of Y will be:
            \begin{equation}
              4 * -\log_2(1/4) = 8
            \end{equation}
            It should be noted, that Y is uniformly distributed and maximizes
            possible entropy.
          \end{itemize}
        \item $H(X,Y)$\\
          Entropy of joint distribution is given by formula:
          \begin{equation}
            H(X, Y) = \sum_{x}\sum_{y}p(x,y) * -\log_2p(x,y)
          \end{equation}
          And equals $\textbf{41/4}$
          \pagebreak

        \item $H(X|Y)$
          Entropy of conditional distribution is given by formula:
          \begin{align}
            H(X|Y) &= \sum_y p(y)H(X|Y=y) \\
                   &= \sum_y p(y) \sum_x p(x|y) * (-\log_2p(x|y))\\
                   &= \sum_y \sum_x p(x,y) * (-\log_2p(x|y))\\
                   &= \sum_y \sum_x p(x,y) * (-\log_2\frac{p(x,y)}{p(x)})
          \end{align}
          Based on the property:
          \begin{equation}
            H(X, Y) = H(X) + H(Y|X)
          \end{equation}
          Conditional entropy is equal to $\textbf{41/4 - 7/4 = 34/4}$
        \item $I(X;Y)$
          Mutual information between random variables X and Y is given by:
          \begin{equation}
          I(X; Y) = \sum_x \sum_y p(x,y)\log_2 \frac{p(x,y)}{p(x)p(y)}
          \end{equation}
          Above formula unrolls into following equation for our distribution:
          \begin{align}
            1/8 * -\log_2(\frac{1/16}{1/8}) + 1/8 * \log_2(\frac{1/8}{1/16}) +
            1/8 * - \log_2(\frac{1/16}{1/32}) + 1/4 * \log_2(\frac{1/4}{1/8}) \\
            1/2 + 1/8 = 5/8
          \end{align}

    \end{itemize}

  \end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
  Assueme we have $e$ attributes, whose distribution is described by random
  variables $X_1, X_2, X_3$ respectively. Attributes fulfill following
  properties:
  \begin{align}
  \end{align}
\end{homeworkProblem}

\pagebreak
% Non sequential homework problems
%
% Jump to problem 18
\begin{homeworkProblem}[18]
\end{homeworkProblem}

% Continue counting to 19

\end{document}
